# ğŸš€ Job Crawling & Analytics Pipeline

An end-to-end **Data Engineering project** that automatically crawls job postings from multiple websites, ingests raw data into **PostgreSQL** and **MinIO**, publishes curated job alerts to **Discord**, and builds a modern **Lakehouse-style Data Warehouse** using **dbt**, **Apache Trino**, and **Iceberg**.

The entire pipeline is orchestrated with **Apache Airflow** and follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** to ensure scalability, data quality, and analytics readiness.

> âš ï¸ This project is built for **learning, experimentation, and portfolio showcase** purposes.

---

## ğŸŒŸ Project Highlights

* ğŸ” **Automated job crawling** from job websites (ITViec, TopCV)
* ğŸ§± **Medallion Architecture** with dbt (Bronze / Silver / Gold)
* â„ï¸ **Lakehouse design** using Iceberg + Trino + MinIO
* ğŸ“£ **Discord job alerts** with deduplication
* ğŸ›  **Airflow orchestration** with modular DAGs
* â˜ï¸ **Object storage** powered by MinIO
* ğŸ”„ **Upsert & incremental processing** for efficiency
* âœ… **Data quality validation** with Great Expectations (pre-staging checks)

## ğŸ¯ Project Objectives

This project aims to:

1. Crawl job postings from multiple sources (currently **ITViec** and **TopCV**, could extend more sources)
2. Store raw and processed data in:

   * **PostgreSQL** (staging / operational layer)
   * **MinIO + Iceberg** (analytical lakehouse)
3. Automatically publish **new job alerts to Discord** without duplicates
4. Transform and model data using **dbt** following Medallion Architecture.
5. Future enhancement features such as:

   * ğŸ¤– Discord chatbot for job recommendations *(maybe when Iâ€™m 60 ğŸ¤£)*

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job Sources    â”‚
â”‚ (ITViec, TopCV) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Crawlers â”‚
â”‚ (Selenium+bs4)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Great Expectation â”‚
â”‚ (Data Validation) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚â”€â”€â”€â”€â”€â–¶â”‚ Discord Channels â”‚
â”‚  (Staging)     â”‚        â”‚   (Job Alerts)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt + Trino   â”‚
â”‚  (Bronze/Silver â”‚
â”‚   / Gold)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BI / Analytics Tools â”‚
â”‚ (Coming soon ğŸ˜…)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Pipeline Flow

### 1ï¸âƒ£ Crawling Stage (Bronze Ingestion)

* **Airflow DAGs**:

  * `itviec_data_pipeline`
  * `topcv_data_pipeline`
* **Trigger**: Manual (for running single source only)
* **Steps**:

  1. Load source URLs from JSON configuration files
  2. Scrape job postings using **Selenium + ChromeDriver**
  3. Extract structured job information
  4. **Validate raw data with Great Expectations (per source)**
  5. Upsert records into PostgreSQL staging tables
  6. Identify unposted jobs
  7. Publish jobs to Discord
  8. Mark jobs as posted to avoid duplicates

  * `master_dag`
* **Trigger**: Daily
* **Steps**:

  1. Run topcv and itviec pipeline (scrape data and ingest into staging layer).
  2. Post unposted jobs to Discord channel and run dbt pipeline for data lakehouse data.

---

### 2ï¸âƒ£ Staging Layer (PostgreSQL)

* **Schema**: `staging`
* **Purpose**: Operational storage
* **Key Features**:

  * `ON CONFLICT` upsert logic
  * `posted_to_discord` flag for idempotency

**Tables**:

* `staging.itviec_data_job`
* `staging.topcv_data_job`

---

### 3ï¸âƒ£ Transformation Layer (dbt â€“ Medallion Architecture)

#### ğŸ¥‰ Bronze Layer (`models/bronze/`)

* Raw data mirrored from PostgreSQL staging
* Minimal transformations
* **Materialization**: Incremental tables

#### ğŸ¥ˆ Silver Layer (`models/silver/`)

* Cleaned & standardized data
* Deduplication, normalization, validation
* **Materialization**: Tables

#### ğŸ¥‡ Gold Layer (`models/gold/`)

* Business-ready analytical models
* Optimized for reporting & insights
* **Materialization**: Tables

ğŸ“˜ Get more detail documentation with:

```bash
dbt docs serve --port 8085
```

---

## ğŸ“£ Discord Integration

* Automatically posts **new job alerts**
* Rich **Discord embeds** with:
  * Job title
  * Company
  * Location
  * Salary (if available)
* Built-in:
  * Error handling
  * Logging
  * Duplicate prevention

---


## â± Airflow DAGs Overview

### `itviec_data_pipeline`

* **Schedule**: Manual Run (configurable - for specific run only - for testing - ...)
* **Tasks**:

  1. Load URLs
  2. Scrape jobs
  3. Insert / upsert jobs
  4. Post new jobs to Discord

### `topcv_data_pipeline`

* Same structure as ITViec pipeline

---

## ğŸ§ª Testing

Basic test scripts are available under `scripts/test/`:

* `test_crawl_it_viec.py`
* `test_crawl_topcv.py`
* `test_db_conn.py`
* ...

> These tests mainly ensure scripts are **runnable**, not full unit tests.

## ğŸ‘¤ Author

**Bao Phan (HarrySon)**

---

â­ If you find this project useful, **please give the repo a star** â€” its much helps! ğŸ˜‹ğŸ”¥
